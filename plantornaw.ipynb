{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "config = {\n",
    "    \"DATA_DIR_PATH\": getenv(\"DATA_DIR_PATH\"),\n",
    "    \"SAVE_TORCH_DIR_PATH\": getenv(\"SAVE_TORCH_DIR_PATH\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.utils.data is a PyTorch method for importing data\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import aggregate_xml_data as ml_helpers\n",
    "from PIL import Image, ImageMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pandas import read_xml\n",
    "\n",
    "\n",
    "#this class is probably just holding a dataframe\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, data_label_filepath, transform=None, target_transform=None):\n",
    "        self.img_labels = read_xml(data_label_filepath, xpath=\"//Data//Image\")\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.img_labels.iloc[idx, 0]\n",
    "        #idx 12 is the <content-mapping> tag\n",
    "        img_dir = self.img_labels.iloc[idx, 11]\n",
    "        img_path = f'{img_dir}/{img_filename}'\n",
    "        absolute_img_path = os.path.join(self.data_dir, img_path)\n",
    "\n",
    "        image = Image.open(absolute_img_path)\n",
    "        #change label index here (0 is filename)\n",
    "        label = self.img_labels.iloc[idx, 12]\n",
    "        # label = {\n",
    "        #     \"filename\": self.img_labels.iloc[idx, 0],\n",
    "        #     \"name\": self.img_labels.iloc[idx, 1],\n",
    "        #     \"author\": self.img_labels.iloc[idx, 3]\n",
    "        # }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all images, define full dataset, split full set into test and train, load test/train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, CenterCrop, Grayscale, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import aggregate_xml_data as ml_helpers\n",
    "\n",
    "\n",
    "train_transform = Compose([Grayscale(num_output_channels=1),Resize(224), CenterCrop(224), ToTensor()]) \n",
    "## changed size from 28 (model trained on this size) to 224 to check reversion to pic from tensor\n",
    "\n",
    "\n",
    "target_transform = int\n",
    "# target_transform = Compose([Resize(255), ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD label file\n",
    "# training_data = ImageFolder(root=\"plant_data\", transform=transform)\n",
    "# target_data = ImageFolder(root=\"plant_data\", target_transform=target_transform)\n",
    "data_dir = config[\"DATA_DIR_PATH\"]\n",
    "data_label_file = \"plant_labels.xml\"\n",
    "# image_labels_filepath = ml_helpers.aggregate_xml_data(data_dir, data_label_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example  encodedimage data\n",
    "{\n",
    "    \"1289495\": 'i-am-encoded-image-data',\n",
    "    \"8954845\": 'i-am-encoded-image-data'\n",
    "}\n",
    "\n",
    "# example image label data\n",
    "[\n",
    "    {\n",
    "        \"PlantId\": 1289495,\n",
    "        # other label fields\n",
    "    },\n",
    "    {\n",
    "        \"PlantId\": 8954845,\n",
    "        # other label fields\n",
    "    }\n",
    "]\n",
    "\n",
    "# final - image label + encoded image\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"PlantId\": 1289495,\n",
    "        \"ImageData\": 'i-am-encoded-image-data'\n",
    "        # other label fields\n",
    "    },\n",
    "    {\n",
    "        \"PlantId\": 8954845,\n",
    "        \"ImageData\": 'i-am-encoded-image-data'\n",
    "        # other label fields\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(image_labels_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DEFINE dataset using custom class in order to try and incorporate label data\n",
    "full_dataset = CustomImageDataset(data_dir, \"image_labels_filepath\", train_transform, target_transform)\n",
    "#SPLIT INTO TEST AND TRAIN SUBSETS\n",
    "\n",
    "print('This is the length of the full data set:', len(full_dataset))\n",
    "\n",
    "train_set_size = int(len(full_dataset) * 0.8)\n",
    "test_set_size = int(len(full_dataset) * 0.1)\n",
    "validation_set_size = int(len(full_dataset))-train_set_size-test_set_size\n",
    "train_set, test_set, validation_set = random_split(full_dataset, [train_set_size, test_set_size, validation_set_size])\n",
    "print(train_set)\n",
    "print(test_set)\n",
    "print(validation_set)\n",
    "   \n",
    "# After\n",
    "print('='*30)\n",
    "print('Train data set:', len(train_set))\n",
    "print('Test data set:', len(test_set))\n",
    "print('Val data set:', len(validation_set))\n",
    "# loading dataset in data loader\n",
    "#dataloader = DataLoader(dataset=training_data, batch_size=4, shuffle=True)\n",
    "train_dataloader = DataLoader(dataset=train_set, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_set, batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, label_names = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {label_names.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#get device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(224*224, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make NN instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameterize the layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 100\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set optimization loop. (These are just instructions, a dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the loss function and optimizer, and pass it to train_loop and test_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, config[\"SAVE_TORCH_DIR_PATH\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import ToPILImage\n",
    "\n",
    "# val_image, val_label = next(iter(validation_set))\n",
    "# post_transform = transforms.Compose([Resize(255), ToPILImage(mode=\"L\")])\n",
    "\n",
    "# final_pic = post_transform(val_image)\n",
    "# final_pic.show()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # print({model(val_image)})\n",
    "#     final_pic = post_transform(model(val_image))\n",
    "#     final_pic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# lmodel = torch.load('<insert-file-path-here>')\n",
    "# val_dataloader = DataLoader(dataset=validation_set, batch_size=3, shuffle=True)\n",
    "# val_img, val_label = next(iter(val_dataloader))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     prediction = lmodel(val_img)\n",
    "#     pred_label = np.argmax(prediction)\n",
    "\n",
    "# print(pred_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# val_dataloader = DataLoader(dataset=validation_set, batch_size=3, shuffle=True)\n",
    "# val_img, val_label = next(iter(val_dataloader))\n",
    "\n",
    "# # CONTENT_LABEL_MAPPING = {\n",
    "# #     \"0\":\"Leaf\",\n",
    "# #     \"1\":\"Flower\",\n",
    "# #     \"2\":\"Entire\",\n",
    "# # }\n",
    "\n",
    "# print(f\"Feature batch shape: {val_img.size()}\")\n",
    "# print(f\"Labels batch shape: {val_label.size()}\")\n",
    "# look_here = val_img.squeeze()\n",
    "# validation_label = val_label[0]\n",
    "\n",
    "\n",
    "# for i in range(3):\n",
    "#     plt.imshow(val_img[i].squeeze())\n",
    "#     print(f\"Label: {val_label[i]}\")\n",
    "#     plt.show(i+1)\n",
    "#     # plt.imshow(np.transpose(val_image.numpy(), (1, 2, 0)))\n",
    "#     # plt.figure(i+1)\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the imported model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 3\n",
    "trained_model = torch.load(config[\"SAVE_TORCH_DIR_PATH\"])\n",
    "val_dataloader = DataLoader(dataset=validation_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_img, val_label = next(iter(val_dataloader))\n",
    "# print('test', val_img[2])\n",
    "# print('this is it:', val_label.size())\n",
    "# for val_img, val_label in val_dataloader:\n",
    "#     print('this is it:', val_label)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = trained_model(val_img[0])\n",
    "    print('my preds: ', prediction)\n",
    "    pred_label = np.argmax(prediction)\n",
    "    # print('return index at prediction: ', val_img[pred_label])\n",
    "    # pred_label = prediction\n",
    "\n",
    "print(f\"Feature batch shape: {val_img.size()}\")\n",
    "print(f\"Labels batch shape: {val_label.size()}\")\n",
    "print(f\"Predicted Labels batch shape: {pred_label.size()}\")\n",
    "# look_here = val_img.squeeze()\n",
    "# validation_label = val_label[0]\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    plt.imshow(val_img[i].squeeze())\n",
    "    print(f\"Label: {val_label[i]}\")\n",
    "    print(val_label.dtype)\n",
    "    # print(f\"Predicted Label: {pred_label.item()}\")\n",
    "    print(f\"Predicted Label: {pred_label}\")\n",
    "    print(pred_label.dtype)\n",
    "    plt.show(i)\n",
    "    # plt.imshow(np.transpose(val_image.numpy(), (1, 2, 0)))\n",
    "    # plt.figure(i+1)\n",
    "# plt.show\n",
    "\n",
    "# plt.imshow(val_img.squeeze())\n",
    "# print(f\"Label: {val_label}\")\n",
    "# print(val_label.dtype)\n",
    "#     # print(f\"Predicted Label: {pred_label.item()}\")\n",
    "# print(f\"Predicted Label: {pred_label}\")\n",
    "# print(pred_label.dtype)\n",
    "#     # plt.imshow(np.transpose(val_image.numpy(), (1, 2, 0)))\n",
    "#     # plt.figure(i+1)\n",
    "# plt.show\n",
    "\n",
    "val_label == pred_label"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
